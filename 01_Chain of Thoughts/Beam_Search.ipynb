{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2304250",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate --quite\n",
    "!pip install networkx --quite\n",
    "import networkx as nx \n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "!apt install graphviz graphviz-dev -y \n",
    "\n",
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcdc7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as from\n",
    "\n",
    "reasoning_model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "reasoning_tokenizer = AutoTokenizer.from_pretrained(reasoning_model_id,use_fast=True)\n",
    "reasoning_model = AutoModelForCausalLM.from_pretrained(\n",
    "    reasoning_model_id,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "reasoning_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb53bd4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load reward model (PRM approximation)\n",
    "reward_model_id = \"OpenAssistant/reward-model-deberta-v3-large\"\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_id)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_id).to(\"cuda\")\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fd0f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate stepwise reward using a final answer reward model\n",
    "\n",
    "def stepwise_prm_score(prompt , trace, reward_model,tokenizer,device = \"cuda\"):\n",
    "    steps = trace.split(\". \")\n",
    "    cumulative_score = 0.0\n",
    "    for i in range(1,len(steps)+1):\n",
    "        partial = prompt + \"\\n\" + \". \".join(steps[:i])\n",
    "        inputs = tokenizer(partial,return_tensors=\"pt\",truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            score = reward_model(**inputs).logits[0].item()\n",
    "        cumulative_score += score\n",
    "    return cumulative_score / len(steps) if steps else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b361398",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def beam_search_with_prm(prompt, reasoning_model, reasoning_tokenizer,\n",
    "                         reward_model, reward_tokenizer,\n",
    "                         N=4, M=2, max_steps=3):\n",
    "    assert N % M == 0, \"N must be divisible by M\"\n",
    "    device = reasoning_model.device\n",
    "\n",
    "    def format_zephyr_prompt(user_prompt: str) -> str:\n",
    "        return f\"<|system|>\\nYou are a helpful assistant.\\n<|user|>\\n{user_prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "    formatted_prompt = format_zephyr_prompt(prompt)\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    node_counter = 0\n",
    "    beams = []\n",
    "\n",
    "    # Step 0: Initial N completions\n",
    "    input_ids = reasoning_tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = reasoning_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        num_beams=N,\n",
    "        temperature=0.9,\n",
    "        num_return_sequences=N,\n",
    "        pad_token_id=reasoning_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    for i in range(N):\n",
    "        gen_text = reasoning_tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        completion = gen_text.replace(formatted_prompt, \"\").strip()\n",
    "        score = stepwise_prm_score(prompt, completion, reward_model, reward_tokenizer)\n",
    "        node_id = f\"0-{i}\"\n",
    "        graph.add_node(node_id, label=completion[:40]+\"...\", score=score)\n",
    "        beams.append((gen_text, score, node_id))\n",
    "\n",
    "    # Steps 1 to max_steps\n",
    "    for step in range(1, max_steps + 1):\n",
    "        beams = sorted(beams, key=lambda x: x[1], reverse=True)[:M]\n",
    "        candidates = []\n",
    "\n",
    "        for parent_text, _, parent_id in beams:\n",
    "            input_ids = reasoning_tokenizer(parent_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "            children = reasoning_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=64,\n",
    "                do_sample=False,\n",
    "                num_beams=(N // M),\n",
    "                num_return_sequences=(N // M),\n",
    "                pad_token_id=reasoning_tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            for i in range(N // M):\n",
    "                child_text = reasoning_tokenizer.decode(children[i], skip_special_tokens=True)\n",
    "                continuation = child_text.replace(parent_text, \"\").strip()\n",
    "                score = stepwise_prm_score(prompt, continuation, reward_model, reward_tokenizer)\n",
    "                node_id = f\"{step}-{i}-{parent_id}\"\n",
    "                graph.add_node(node_id, label=continuation[:40]+\"...\", score=score)\n",
    "                graph.add_edge(parent_id, node_id)\n",
    "                candidates.append((child_text, score, node_id))\n",
    "\n",
    "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "\n",
    "    return beams, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c32b9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_trace_graph_tree_clean(graph, figsize=(14, 8), title=\"Beam Search Tree (PRM-Guided)\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    import networkx as nx\n",
    "    import textwrap\n",
    "\n",
    "    try:\n",
    "        pos = nx.nx_agraph.graphviz_layout(graph, prog='dot')\n",
    "    except:\n",
    "        pos = nx.spring_layout(graph, seed=42)\n",
    "\n",
    "    scores = nx.get_node_attributes(graph, 'score')\n",
    "    labels = nx.get_node_attributes(graph, 'label')\n",
    "    node_colors = [scores[n] for n in graph.nodes()]\n",
    "    node_order = list(graph.nodes())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(graph, pos, ax=ax, alpha=0.3)\n",
    "\n",
    "    # Draw nodes with color based on PRM score (no edge color)\n",
    "    for node in node_order:\n",
    "        x, y = pos[node]\n",
    "        score = scores.get(node, 0)\n",
    "        fill_color = plt.cm.viridis((score - min(node_colors)) / (max(node_colors) - min(node_colors)))\n",
    "\n",
    "        ax.scatter(x, y, s=800, c=[fill_color], edgecolors='black', linewidths=1, zorder=5)\n",
    "\n",
    "        # Shortened label below node\n",
    "        text = textwrap.shorten(labels.get(node, \"\"), width=50, placeholder=\"...\")\n",
    "        ax.text(x, y - 30, text, ha='center', va='top', fontsize=8,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"black\", lw=0.4), zorder=10)\n",
    "\n",
    "    # Colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis,\n",
    "                               norm=plt.Normalize(vmin=min(node_colors), vmax=max(node_colors)))\n",
    "    sm.set_array(node_colors)\n",
    "    fig.colorbar(sm, ax=ax, label=\"PRM Score\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e7876",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Roger has 5 tennis balls. He buys 2 cans of 3 tennis balls each. How many tennis balls does he have now?\"\n",
    "beams, graph = beam_search_with_prm(prompt, reasoning_model, reasoning_tokenizer, reward_model, reward_tokenizer, N=4, M=2, max_steps=3)\n",
    "\n",
    "for i, (text, score, _) in enumerate(beams):\n",
    "    print(f\"--- Final Beam {i+1} ---\\nScore: {score:.2f}\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f110d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba4a8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d66b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82ec3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8550b1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5790a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
