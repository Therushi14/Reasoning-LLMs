{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svHy8z6PTK9p"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gsm8k = load_dataset(\"gsm8k\", \"main\",split=\"test[:50]\")"
      ],
      "metadata": {
        "id": "rgNnHUJNTYOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/arkilpatel/SVAMP.git\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "with open(\"SVAMP/SVAMP.json\", \"r\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "svamp = Dataset.from_list([\n",
        "    {\"Question\":item[\"Body\"],\"Answer\":str(item[\"Answer\"])}\n",
        "    for item in raw_data\n",
        "])"
      ],
      "metadata": {
        "id": "V-sgGa-3TYK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svamp_subset = svamp.select(range(50))"
      ],
      "metadata": {
        "id": "UZ8fKze8TYIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Few-shot Chain of Thought prompt examples\n",
        "few_shot_prefix = \"\"\"Q: Emily has 3 apples. Her friend gives her 2 more. How many apples does Emily have now?\n",
        "A: Emily starts with 3 apples. Her friend gives her 2 more. So, 3 + 2 = 5. The answer is 5.\n",
        "\n",
        "Q: A pen costs 2 dollars. John buys 4 pens. How much does he pay?\n",
        "A: Each pen costs 2 dollars. John buys 4 pens. So, 2 Ã— 4 = 8. The answer is 8.\n",
        "\n",
        "Q: Jake read 5 pages on Monday and 7 pages on Tuesday. How many pages did he read in total?\n",
        "A: Jake read 5 pages on Monday and 7 on Tuesday. So, 5 + 7 = 12. The answer is 12.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "-etKQ-8KTYFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer , AutoModelForSeq2SeqLM, pipeline\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def evaluate_model_fewshot(model_id):\n",
        "  print(f\"Evaluation {model_id} with few-shot CoT...\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "  pipe = pipeline(\"text2text-generation\",model = model , tokenizer = tokenizer , max_new_tokens = 128)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for sample in gsm8k:\n",
        "    question = sample[\"question\"]\n",
        "    gt_answer = sample[\"answer\"].split(\"####\")[-1].strip()\n",
        "\n",
        "    prompt = few_shot_prefix + f\"QL {question}\\nA:\"\n",
        "    output = pipe(prompt)[0][\"generated_text\"]\n",
        "\n",
        "    match = re.search(r\"(\\d+(?:\\.\\d+)?)\", output.replace(\",\", \"\"))\n",
        "    if match:\n",
        "      pred = match.group(1)\n",
        "      if pred == gt_answer:\n",
        "        correct += 1\n",
        "\n",
        "      total += 1\n",
        "\n",
        "    acc = correct/ total\n",
        "    print(f\"Accuracy: {acc:.4}\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "_EyHmDM7TX9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_ids = {\n",
        "    \"Flan-T5 Small\": \"google/flan-t5-small\",\n",
        "    \"Flan-T5 Base\": \"google/flan-t5-base\",\n",
        "    \"Flan-T5 Large\": \"google/flan-t5-large\"\n",
        "}\n",
        "\n",
        "results = []\n",
        "for label , model in model_ids.items():\n",
        "  acc = evaluate_model_fewshot(model)\n",
        "  results.append((label,acc))\n",
        "\n",
        "labels = [r[0] for r in results]\n",
        "scores = [r[1] for r in results]\n",
        "x = range(len(labels))\n",
        "\n",
        "plt.figure(figsize = (8,5))\n",
        "plt.bar(x,scores , color = \"skyblue\")\n",
        "plt.xticks(x,labels)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Few-shot CoT Reasoning vs Model Size (GSM8K)\")\n",
        "plt.ylim(0,1.0)\n",
        "plt.grid(True,axis = 'y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ekMNj3NDTX6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_five = gsm8k.select(range(5))  # You can change the range if needed\n",
        "\n",
        "def print_gsm8k_predictions(model_id, dataset, few_shot_prefix, decoder_only=False):\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        AutoModelForSeq2SeqLM,\n",
        "        AutoModelForCausalLM,\n",
        "        pipeline\n",
        "    )\n",
        "    import re\n",
        "    import torch\n",
        "\n",
        "    # Load appropriate model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    if decoder_only:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "        )\n",
        "        task = \"text-generation\"\n",
        "    else:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "        task = \"text2text-generation\"\n",
        "\n",
        "    pipe = pipeline(task, model=model, tokenizer=tokenizer, max_new_tokens=128, temperature=0.3)\n",
        "\n",
        "    def extract_answer(text):\n",
        "        text = text.replace(\",\", \"\")\n",
        "        numbers = re.findall(r\"[-+]?\\d+(?:\\.\\d+)?\", text)\n",
        "        return numbers[-1] if numbers else None\n",
        "\n",
        "    for i, sample in enumerate(dataset):\n",
        "        question = sample[\"question\"]\n",
        "        gt_answer = sample[\"answer\"].split(\"####\")[-1].strip()\n",
        "\n",
        "        prompt = few_shot_prefix + f\"\\nQ: {question}\\nA:\"\n",
        "        output = pipe(prompt)[0][\"generated_text\"]\n",
        "        if decoder_only:\n",
        "            output_text = output.split(\"A:\")[-1].strip()\n",
        "        else:\n",
        "            output_text = output.strip()\n",
        "\n",
        "        pred = extract_answer(output_text)\n",
        "\n",
        "        print(f\"\\nðŸ”¹ Example {i + 1}\")\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"GT Answer: {gt_answer}\")\n",
        "        print(f\"Model Output:\\n{output_text}\")\n",
        "        print(f\"Extracted Answer: {pred}\")\n",
        "\n",
        "\n",
        "print_gsm8k_predictions(\"google/flan-t5-base\", gsm8k_five, few_shot_prefix, decoder_only=False)\n"
      ],
      "metadata": {
        "id": "efofEOCITX4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bigger** **Models**"
      ],
      "metadata": {
        "id": "8ljML58fkR0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer , AutoModelForCasualLM ,pipeline\n",
        "import re\n",
        "import torch\n",
        "\n",
        "\n",
        "def evaluate_decoder_model(model_id,dataset,few_shot_prefix):\n",
        "  print(f\"Evaluating  {model_id} on GSM8K...\")\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  model = AutoModelForCasualLM.from_pretrained(\n",
        "      model_id,\n",
        "      device_map = \"auto\",\n",
        "      torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "  )\n",
        "\n",
        "  pipe = pipeline(\"text-generation\",model = model , tokenizer = tokenizer ,max_new_tokens = 128,temperature = 0.3)\n",
        "\n",
        "  def extract_answer(text):\n",
        "    text = text.replace(\",\",\"\")\n",
        "    numbers = re.findall(r\"[-+]?\\d+(?:\\.\\d+)?\", text)\n",
        "    return numbers[-1] if numbers else None\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for sample in dataset:\n",
        "    question = sample[\"question\"]\n",
        "    gt_answer = sample[\"answer\"].split(\"####\")[-1].strip()\n",
        "\n",
        "    prompt = few_shot_prefix + f\"\\nQ: {question}\\nA:\"\n",
        "    output = pipe(prompt)[0][\"generated_text\"]\n",
        "    output_text = output.split(\"A:\")[-1].strip()\n",
        "    pred = extract_answer(output_text)\n",
        "\n",
        "    if pred and pred.strip().lstrip(\"0\") == gt_answer.strip().lstrip(\"0\"):\n",
        "      correct += 1\n",
        "    total += 1\n",
        "\n",
        "  acc = correct / total\n",
        "  print(f\"Accuracy: {acc:.4}\")\n",
        "  return acc\n"
      ],
      "metadata": {
        "id": "nBLjGE7RTX1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_models = {\n",
        "    \"Zephyr-7B\": \"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    \"Phi-2\": \"microsoft/phi-2\",\n",
        "    \"TinyLlama-1.1B\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "}\n",
        "\n",
        "decoder_results = []\n",
        "for label, model_id in decoder_models.items():\n",
        "    acc = evaluate_decoder_model(model_id, gsm8k, few_shot_prefix)\n",
        "    decoder_results.append((label, acc))"
      ],
      "metadata": {
        "id": "AFDc9RuTTXy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gsm8k_predictions(model_id, dataset, few_shot_prefix):\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "    import re\n",
        "    import torch\n",
        "\n",
        "    print(f\"\\n Predictions from {model_id} on 5 GSM8K Questions\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "    )\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128, temperature=0.3)\n",
        "\n",
        "    def extract_answer(text):\n",
        "        text = text.replace(\",\", \"\")\n",
        "        numbers = re.findall(r\"[-+]?\\d+(?:\\.\\d+)?\", text)\n",
        "        return numbers[-1] if numbers else None\n",
        "\n",
        "    for i, sample in enumerate(dataset.select(range(5))):  # first 5 samples\n",
        "        question = sample[\"question\"]\n",
        "        gt_answer = sample[\"answer\"].split(\"####\")[-1].strip()\n",
        "\n",
        "        prompt = few_shot_prefix + f\"\\nQ: {question}\\nA:\"\n",
        "        output = pipe(prompt)[0][\"generated_text\"]\n",
        "        output_text = output.split(\"A:\")[-1].strip()\n",
        "        pred = extract_answer(output_text)\n",
        "\n",
        "        print(f\"\\n Example {i + 1}\")\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"GT Answer: {gt_answer}\")\n",
        "        print(f\"Model Output:\\n{output_text}\")\n",
        "        print(f\"Extracted Answer: {pred}\")\n",
        "\n",
        "# Zephyr\n",
        "print_gsm8k_predictions(\"HuggingFaceH4/zephyr-7b-alpha\", gsm8k, few_shot_prefix)\n",
        "\n",
        "# Phi-2\n",
        "print_gsm8k_predictions(\"microsoft/phi-2\", gsm8k, few_shot_prefix)\n",
        "\n",
        "# TinyLlama\n",
        "print_gsm8k_predictions(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", gsm8k, few_shot_prefix)\n"
      ],
      "metadata": {
        "id": "8VGpDNQDTXvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine model labels, sizes, and accuracy\n",
        "all_results = results + decoder_results\n",
        "\n",
        "# Define size annotations (approximate)\n",
        "model_sizes = {\n",
        "    \"Flan-T5 Small\": \"80M\",\n",
        "    \"Flan-T5 Base\": \"250M\",\n",
        "    \"Flan-T5 Large\": \"800M\",\n",
        "    \"Zephyr-7B\": \"7B\",\n",
        "    \"Phi-2\": \"2.7B\",\n",
        "    \"TinyLlama-1.1B\": \"1.1B\"\n",
        "}\n",
        "\n",
        "labels = [f\"{name}\\n({model_sizes[name]})\" for name, _ in all_results]\n",
        "scores = [score for _, score in all_results]\n",
        "x = range(len(labels))\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(x, scores, color=\"mediumseagreen\")\n",
        "plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Few-Shot CoT Reasoning vs Model Size (GSM8K)\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.grid(True, axis=\"y\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f00kouY0TXtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjcJT_R5TXqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCJ0HmUPTXoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6SCBuw8TXlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MF2_E0x2TXjH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}